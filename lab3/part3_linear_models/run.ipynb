{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Walk Through Linear Models\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission.*\n",
    "\n",
    "In this exercise you will:\n",
    "    \n",
    "- implement a whole bunch of **linear classifiers**\n",
    "- compare their performance and properties\n",
    "\n",
    "Please note that **YOU CANNOT USE ANY MACHINE LEARNING PACKAGE SUCH AS SKLEARN** for any homework, unless you are asked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic imports\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from mkdata import mkdata\n",
    "from plotdata import plotdata\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this skeleton or write your own.\n",
    "\n",
    "NOTE: Be becareful that the bias term is in the first element of weight, that is `y = np.sign(np.matmul(w_g.T, np.vstack((np.ones((1, X.shape[1])), X)))).T`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_regression import linear_regression\n",
    "\n",
    "nRep = 1000  # number of replicates\n",
    "nTrain = 100 # number of training data\n",
    "\n",
    "E_train_sum = 0\n",
    "E_test_sum = 0\n",
    "\n",
    "for i in range(nRep):\n",
    "    X, y, w_f = mkdata(nTrain)\n",
    "    w_g = linear_regression(X, y)\n",
    "    \n",
    "    # Compute training error\n",
    "    y_pred_train = np.sign(np.matmul(w_g.T, np.vstack((np.ones((1, X.shape[1])), X))))\n",
    "    E_train = np.mean(y_pred_train != y)\n",
    "    E_train_sum += E_train\n",
    "    \n",
    "    # Generate test data and compute testing error\n",
    "    X_test, y_test, _ = mkdata(10000)  # Large test set\n",
    "    y_pred_test = np.sign(np.matmul(w_g.T, np.vstack((np.ones((1, X_test.shape[1])), X_test))))\n",
    "    E_test = np.mean(y_pred_test != y_test)\n",
    "    E_test_sum += E_test\n",
    "\n",
    "E_train_avg = E_train_sum / nRep\n",
    "E_test_avg = E_test_sum / nRep\n",
    "\n",
    "print('E_train is {}, E_test is {}'.format(E_train_avg, E_test_avg))\n",
    "\n",
    "plotdata(X, y, w_f, w_g, 'Linear Regression');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2: Linear Regression: noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nRep = 1000  # number of replicates\n",
    "nTrain = 100  # number of training data\n",
    "\n",
    "E_train_sum = 0\n",
    "E_test_sum = 0\n",
    "\n",
    "for i in range(nRep):\n",
    "    X, y, w_f = mkdata(nTrain, 'noisy')\n",
    "    w_g = linear_regression(X, y)\n",
    "    \n",
    "    # Compute training error\n",
    "    y_pred_train = np.sign(np.matmul(w_g.T, np.vstack((np.ones((1, X.shape[1])), X))))\n",
    "    E_train = np.mean(y_pred_train != y)\n",
    "    E_train_sum += E_train\n",
    "    \n",
    "    # Generate test data and compute testing error\n",
    "    X_test, y_test, _ = mkdata(10000, 'noisy')\n",
    "    y_pred_test = np.sign(np.matmul(w_g.T, np.vstack((np.ones((1, X_test.shape[1])), X_test))))\n",
    "    E_test = np.mean(y_pred_test != y_test)\n",
    "    E_test_sum += E_test\n",
    "\n",
    "E_train_avg = E_train_sum / nRep\n",
    "E_test_avg = E_test_sum / nRep\n",
    "\n",
    "print('E_train is {}, E_test is {}'.format(E_train_avg, E_test_avg))\n",
    "\n",
    "plotdata(X, y, w_f, w_g, 'Linear Regression: noisy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part3: Linear Regression: poly_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "poly_train = sio.loadmat('poly_train')\n",
    "X, y = poly_train['X'], poly_train['y']\n",
    "poly_test = sio.loadmat('poly_test')\n",
    "X_test, y_test = poly_test['X_test'], poly_test['y_test']\n",
    "\n",
    "# Without transformation\n",
    "w = linear_regression(X, y)\n",
    "\n",
    "# Compute training error\n",
    "y_pred_train = np.sign(np.matmul(w.T, np.vstack((np.ones((1, X.shape[1])), X))))\n",
    "E_train = np.mean(y_pred_train != y)\n",
    "\n",
    "# Compute testing error\n",
    "y_pred_test = np.sign(np.matmul(w.T, np.vstack((np.ones((1, X_test.shape[1])), X_test))))\n",
    "E_test = np.mean(y_pred_test != y_test)\n",
    "\n",
    "print('Without transformation: E_train is {}, E_test is {}'.format(E_train, E_test))\n",
    "\n",
    "# With polynomial transformation (degree 2)\n",
    "X_t = np.vstack((X, X[0]**2, X[1]**2, X[0]*X[1]))  # Add quadratic features\n",
    "X_test_t = np.vstack((X_test, X_test[0]**2, X_test[1]**2, X_test[0]*X_test[1]))\n",
    "\n",
    "w = linear_regression(X_t, y)\n",
    "\n",
    "# Compute training error\n",
    "y_pred_train = np.sign(np.matmul(w.T, np.vstack((np.ones((1, X_t.shape[1])), X_t))))\n",
    "E_train = np.mean(y_pred_train != y)\n",
    "\n",
    "# Compute testing error\n",
    "y_pred_test = np.sign(np.matmul(w.T, np.vstack((np.ones((1, X_test_t.shape[1])), X_test_t))))\n",
    "E_test = np.mean(y_pred_test != y_test)\n",
    "\n",
    "print('With polynomial transformation: E_train is {}, E_test is {}'.format(E_train, E_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part4: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logistic import logistic\n",
    "\n",
    "# Since logistic regression outputs 0/1, we should adjust the label y.\n",
    "nRep = 100  # number of replicates\n",
    "nTrain = 100  # number of training data\n",
    "\n",
    "E_train_sum = 0\n",
    "E_test_sum = 0\n",
    "\n",
    "for i in range(nRep):\n",
    "    X, y, w_f = mkdata(nTrain)\n",
    "    # Convert labels from {-1, 1} to {0, 1} for logistic regression\n",
    "    y_logistic = (y + 1) / 2\n",
    "    w_g = logistic(X, y_logistic)\n",
    "    \n",
    "    # Compute training error (convert back to {-1, 1})\n",
    "    predictions = np.matmul(w_g.T, np.vstack((np.ones((1, X.shape[1])), X)))\n",
    "    y_pred_train = (predictions > 0).astype(int) * 2 - 1\n",
    "    E_train = np.mean(y_pred_train != y)\n",
    "    E_train_sum += E_train\n",
    "    \n",
    "    # Generate test data and compute testing error\n",
    "    X_test, y_test, _ = mkdata(10000)\n",
    "    predictions = np.matmul(w_g.T, np.vstack((np.ones((1, X_test.shape[1])), X_test)))\n",
    "    y_pred_test = (predictions > 0).astype(int) * 2 - 1\n",
    "    E_test = np.mean(y_pred_test != y_test)\n",
    "    E_test_sum += E_test\n",
    "\n",
    "E_train_avg = E_train_sum / nRep\n",
    "E_test_avg = E_test_sum / nRep\n",
    "\n",
    "print('E_train is {}, E_test is {}'.format(E_train_avg, E_test_avg))\n",
    "\n",
    "plotdata(X, y, w_f, w_g, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part5: Logistic Regression: noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Since logistic regression outputs 0/1, we should adjust the label y.\n",
    "nRep = 100  # number of replicates\n",
    "nTrain = 100  # number of training data\n",
    "nTest = 10000  # number of test data\n",
    "\n",
    "E_train_sum = 0\n",
    "E_test_sum = 0\n",
    "\n",
    "for i in range(nRep):\n",
    "    X, y, w_f = mkdata(nTrain, 'noisy')\n",
    "    # Convert labels from {-1, 1} to {0, 1} for logistic regression\n",
    "    y_logistic = (y + 1) / 2\n",
    "    w_g = logistic(X, y_logistic)\n",
    "    \n",
    "    # Compute training error (convert back to {-1, 1})\n",
    "    predictions = np.matmul(w_g.T, np.vstack((np.ones((1, X.shape[1])), X)))\n",
    "    y_pred_train = (predictions > 0).astype(int) * 2 - 1\n",
    "    E_train = np.mean(y_pred_train != y)\n",
    "    E_train_sum += E_train\n",
    "    \n",
    "    # Generate test data and compute testing error\n",
    "    X_test, y_test, _ = mkdata(nTest, 'noisy')\n",
    "    predictions = np.matmul(w_g.T, np.vstack((np.ones((1, X_test.shape[1])), X_test)))\n",
    "    y_pred_test = (predictions > 0).astype(int) * 2 - 1\n",
    "    E_test = np.mean(y_pred_test != y_test)\n",
    "    E_test_sum += E_test\n",
    "\n",
    "E_train_avg = E_train_sum / nRep\n",
    "E_test_avg = E_test_sum / nRep\n",
    "\n",
    "print('E_train is {}, E_test is {}'.format(E_train_avg, E_test_avg))\n",
    "\n",
    "plotdata(X, y, w_f, w_g, 'Logistic Regression: noisy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part6: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svm import svm\n",
    "\n",
    "nRep = 1000  # number of replicates\n",
    "nTrain = 100  # number of training data\n",
    "\n",
    "E_train_sum = 0\n",
    "E_test_sum = 0\n",
    "num_sv_sum = 0\n",
    "\n",
    "for i in range(nRep):\n",
    "    X, y, w_f = mkdata(nTrain)\n",
    "    w_g, num_sv = svm(X, y)\n",
    "    \n",
    "    # Compute training error\n",
    "    y_pred_train = np.sign(np.matmul(w_g.T, np.vstack((np.ones((1, X.shape[1])), X))))\n",
    "    E_train = np.mean(y_pred_train != y)\n",
    "    E_train_sum += E_train\n",
    "    \n",
    "    # Generate test data and compute testing error\n",
    "    X_test, y_test, _ = mkdata(10000)\n",
    "    y_pred_test = np.sign(np.matmul(w_g.T, np.vstack((np.ones((1, X_test.shape[1])), X_test))))\n",
    "    E_test = np.mean(y_pred_test != y_test)\n",
    "    E_test_sum += E_test\n",
    "    \n",
    "    num_sv_sum += num_sv\n",
    "\n",
    "E_train_avg = E_train_sum / nRep\n",
    "E_test_avg = E_test_sum / nRep\n",
    "avgNum = num_sv_sum / nRep\n",
    "\n",
    "print('E_train is {}, E_test is {}'.format(E_train_avg, E_test_avg))\n",
    "print('Average number of support vectors is {}.'.format(avgNum))\n",
    "\n",
    "plotdata(X, y, w_f, w_g, 'SVM')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
